{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc, os\n",
    "import logging\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "data_path = 'C:/Users/cross/Desktop/team-learning-data-mining-Datawhale/NewsRecommender/data_raw/'\n",
    "save_path = 'C:/Users/cross/Desktop/team-learning-data-mining-Datawhale/NewsRecommender/tmp_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 节省内存的一个函数\n",
    "# 减少内存\n",
    "def reduce_mem(df):\n",
    "    starttime = time.time()\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if pd.isnull(c_min) or pd.isnull(c_max):\n",
    "                continue\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('-- Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction),time spend:{:2.2f} min'.format(end_mem,\n",
    "                                                                                                           100*(start_mem-end_mem)/start_mem,\n",
    "                                                                                                           (time.time()-starttime)/60))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据读取\n",
    "### 训练和验证集的划分\n",
    "划分训练和验证集的原因是为了在线下验证模型参数的好坏，为了完全模拟测试集，我们这里就在训练集中抽取部分用户的所有信息来作为验证集。提前做训练验证集划分的好处就是可以分解制作排序特征时的压力，一次性做整个数据集的排序特征可能时间会比较长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_click_df指的是训练集\n",
    "# sample_user_nums 采样作为验证集的用户数量\n",
    "def trn_val_split(all_click_df, sample_user_nums):\n",
    "    all_click = all_click_df\n",
    "    all_user_ids = all_click.user_id.unique()\n",
    "    \n",
    "    # replace=True表示可以重复抽样，反之不可以\n",
    "    sample_user_ids = np.random.choice(all_user_ids, size=sample_user_nums, replace=False) \n",
    "    \n",
    "    click_val = all_click[all_click['user_id'].isin(sample_user_ids)]\n",
    "    click_trn = all_click[~all_click['user_id'].isin(sample_user_ids)]\n",
    "    \n",
    "    # 将验证集中的最后一次点击给抽取出来作为答案\n",
    "    click_val = click_val.sort_values(['user_id', 'click_timestamp'])\n",
    "    val_ans = click_val.groupby('user_id').tail(1)\n",
    "    \n",
    "    click_val = click_val.groupby('user_id').apply(lambda x: x[:-1]).reset_index(drop=True)\n",
    "    \n",
    "    # 去除val_ans中某些用户只有一个点击数据的情况，如果该用户只有一个点击数据，又被分到ans中，\n",
    "    # 那么训练集中就没有这个用户的点击数据，出现用户冷启动问题，给自己模型验证带来麻烦\n",
    "    val_ans = val_ans[val_ans.user_id.isin(click_val.user_id.unique())] # 保证答案中出现的用户再验证集中还有\n",
    "    click_val = click_val[click_val.user_id.isin(val_ans.user_id.unique())]\n",
    "    \n",
    "    return click_trn, click_val, val_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取当前数据的历史点击和最后一次点击\n",
    "def get_hist_and_last_click(all_click):\n",
    "    all_click = all_click.sort_values(by=['user_id', 'click_timestamp'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "\n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取训练、验证及测试集\n",
    "def get_trn_val_tst_data(data_path, offline=True):\n",
    "    if offline:\n",
    "        click_trn_data = pd.read_csv(data_path+'train_click_log.csv')  # 训练集用户点击日志\n",
    "        click_trn_data = reduce_mem(click_trn_data)\n",
    "        click_trn, click_val, val_ans = trn_val_split(click_trn_data , sample_user_nums)\n",
    "    else:\n",
    "        click_trn = pd.read_csv(data_path+'train_click_log.csv')\n",
    "        click_trn = reduce_mem(click_trn)\n",
    "        click_val = None\n",
    "        val_ans = None\n",
    "    \n",
    "    click_tst = pd.read_csv(data_path+'testA_click_log.csv')\n",
    "    \n",
    "    return click_trn, click_val, click_tst, val_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回多路召回列表或者单路召回\n",
    "def get_recall_list(save_path, single_recall_model=None, multi_recall=False):\n",
    "    if multi_recall:\n",
    "        return pickle.load(open(save_path + 'final_recall_items_dict.pkl', 'rb'))\n",
    "    \n",
    "    if single_recall_model == 'i2i_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_recall_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'i2i_emb_itemcf':\n",
    "        return pickle.load(open(save_path + 'itemcf_emb_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'user_cf':\n",
    "        return pickle.load(open(save_path + 'youtubednn_usercf_dict.pkl', 'rb'))\n",
    "    elif single_recall_model == 'youtubednn':\n",
    "        return pickle.load(open(save_path + 'youtube_u2i_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03334249, -0.0170078 , -0.04145873,  0.04109193, -0.01407904,\n",
       "        0.03646303,  0.01296026,  0.03917954, -0.00697747,  0.0390469 ,\n",
       "       -0.01993462,  0.03311949], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "docs = [['30760', '157507'],\n",
    "       ['289197', '63746'],\n",
    "       ['36162', '168401'],\n",
    "       ['50644', '36162']]\n",
    "w2v = Word2Vec(docs, size=12, sg=1, window=2, seed=2020, workers=2, min_count=1, iter=1)\n",
    "\n",
    "# 查看'30760'表示的词向量\n",
    "w2v['30760']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trian_item_word2vec(click_df, embed_size=64, save_name='item_w2v_emb.pkl', split_char=' '):\n",
    "    click_df = click_df.sort_values('click_timestamp')\n",
    "    # 只有转换成字符串才可以进行训练\n",
    "    click_df['click_article_id'] = click_df['click_article_id'].astype(str)\n",
    "    # 转换成句子的形式\n",
    "    docs = click_df.groupby(['user_id'])['click_article_id'].apply(lambda x: list(x)).reset_index()\n",
    "    docs = docs['click_article_id'].values.tolist()\n",
    "\n",
    "    # 为了方便查看训练的进度，这里设定一个log信息\n",
    "    logging.basicConfig(format='%(asctime)s:%(levelname)s:%(message)s', level=logging.INFO)\n",
    "\n",
    "    # 这里的参数对训练得到的向量影响也很大,默认负采样为5\n",
    "    w2v = Word2Vec(docs, size=16, sg=1, window=5, seed=2020, workers=24, min_count=1, iter=1)\n",
    "    \n",
    "    # 保存成字典的形式\n",
    "    item_w2v_emb_dict = {k: w2v[k] for k in click_df['click_article_id']}\n",
    "    pickle.dump(item_w2v_emb_dict, open(save_path + 'item_w2v_emb.pkl', 'wb'))\n",
    "    \n",
    "    return item_w2v_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过字典查询对应的item的Embedding\n",
    "def get_embedding(save_path, all_click_df):\n",
    "    if os.path.exists(save_path + 'item_content_emb.pkl'):\n",
    "        item_content_emb_dict = pickle.load(open(save_path + 'item_content_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('item_content_emb.pkl 文件不存在...')\n",
    "        \n",
    "    # w2v Embedding是需要提前训练好的\n",
    "    if os.path.exists(save_path + 'item_w2v_emb.pkl'):\n",
    "        item_w2v_emb_dict = pickle.load(open(save_path + 'item_w2v_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        item_w2v_emb_dict = trian_item_word2vec(all_click_df)\n",
    "        \n",
    "    if os.path.exists(save_path + 'item_youtube_emb.pkl'):\n",
    "        item_youtube_emb_dict = pickle.load(open(save_path + 'item_youtube_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('item_youtube_emb.pkl 文件不存在...')\n",
    "    \n",
    "    if os.path.exists(save_path + 'user_youtube_emb.pkl'):\n",
    "        user_youtube_emb_dict = pickle.load(open(save_path + 'user_youtube_emb.pkl', 'rb'))\n",
    "    else:\n",
    "        print('user_youtube_emb.pkl 文件不存在...')\n",
    "    \n",
    "    return item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取文章信息\n",
    "def get_article_info_df():\n",
    "    article_info_df = pd.read_csv(data_path + 'articles.csv')\n",
    "    article_info_df = reduce_mem(article_info_df)\n",
    "    \n",
    "    return article_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to 23.34 Mb (69.4% reduction),time spend:0.01 min\n"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "# 这里offline的online的区别就是验证集是否为空\n",
    "click_trn, click_val, click_tst, val_ans = get_trn_val_tst_data(data_path, offline=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_trn_hist, click_trn_last = get_hist_and_last_click(click_trn)\n",
    "\n",
    "if click_val is not None:\n",
    "    click_val_hist, click_val_last = click_val, val_ans\n",
    "else:\n",
    "    click_val_hist, click_val_last = None, None\n",
    "    \n",
    "click_tst_hist = click_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对训练数据做负采样\n",
    "通过召回我们将数据转换成三元组的形式（user1, item1, label）的形式，观察发现正负样本差距极度不平衡，我们可以先对负样本进行下采样，下采样的目的一方面缓解了正负样本比例的问题，另一方面也减小了我们做排序特征的压力，我们在做负采样的时候又有哪些东西是需要注意的呢？  \n",
    "\n",
    "1. 只对负样本进行下采样(如果有比较好的正样本扩充的方法其实也是可以考虑的)\n",
    "2. 负采样之后，保证所有的用户和文章仍然出现在采样之后的数据中\n",
    "3. 下采样的比例可以根据实际情况人为的控制\n",
    "4. 做完负采样之后，更新此时新的用户召回文章列表，因为后续做特征的时候可能用到相对位置的信息。\n",
    "\n",
    "其实负采样也可以留在后面做完特征在进行，这里由于做排序特征太慢了，所以把负采样的环节提到前面了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将召回列表转换成df的形式\n",
    "def recall_dict_2_df(recall_list_dict):\n",
    "    df_row_list = [] # [user, item, score]\n",
    "    for user, recall_list in tqdm(recall_list_dict.items()):\n",
    "        for item, score in recall_list:\n",
    "            df_row_list.append([user, item, score])\n",
    "    \n",
    "    col_names = ['user_id', 'sim_item', 'score']\n",
    "    recall_list_df = pd.DataFrame(df_row_list, columns=col_names)\n",
    "    \n",
    "    return recall_list_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 负采样函数，这里可以控制负采样时的比例, 这里给了一个默认的值\n",
    "def neg_sample_recall_data(recall_items_df, sample_rate=0.001):\n",
    "    pos_data = recall_items_df[recall_items_df['label'] == 1]\n",
    "    neg_data = recall_items_df[recall_items_df['label'] == 0]\n",
    "    \n",
    "    print('pos_data_num:', len(pos_data), 'neg_data_num:', len(neg_data), 'pos/neg:', len(pos_data)/len(neg_data))\n",
    "    \n",
    "    # 分组采样函数\n",
    "    def neg_sample_func(group_df):\n",
    "        neg_num = len(group_df)\n",
    "        sample_num = max(int(neg_num * sample_rate), 1) # 保证最少有一个\n",
    "        sample_num = min(sample_num, 5) # 保证最多不超过5个，这里可以根据实际情况进行选择\n",
    "        return group_df.sample(n=sample_num, replace=True)\n",
    "    \n",
    "    # 对用户进行负采样，保证所有用户都在采样后的数据中\n",
    "    neg_data_user_sample = neg_data.groupby('user_id', group_keys=False).apply(neg_sample_func)\n",
    "    # 对文章进行负采样，保证所有文章都在采样后的数据中\n",
    "    neg_data_item_sample = neg_data.groupby('sim_item', group_keys=False).apply(neg_sample_func)\n",
    "    \n",
    "    # 将上述两种情况下的采样数据合并\n",
    "    neg_data_new = neg_data_user_sample.append(neg_data_item_sample)\n",
    "    # 由于上述两个操作是分开的，可能将两个相同的数据给重复选择了，所以需要对合并后的数据进行去重\n",
    "    neg_data_new = neg_data_new.sort_values(['user_id', 'score']).drop_duplicates(['user_id', 'sim_item'], keep='last')\n",
    "    \n",
    "    # 将正样本数据合并\n",
    "    data_new = pd.concat([pos_data, neg_data_new], ignore_index=True)\n",
    "    \n",
    "    return data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 召回数据打标签\n",
    "def get_rank_label_df(recall_list_df, label_df, is_test=False):\n",
    "    # 测试集是没有标签了，为了后面代码同一一些，这里直接给一个负数替代\n",
    "    if is_test:\n",
    "        recall_list_df['label'] = -1\n",
    "        return recall_list_df\n",
    "    \n",
    "    label_df = label_df.rename(columns={'click_article_id': 'sim_item'})\n",
    "    recall_list_df_ = recall_list_df.merge(label_df[['user_id', 'sim_item', 'click_timestamp']], \\\n",
    "                                               how='left', on=['user_id', 'sim_item'])\n",
    "    recall_list_df_['label'] = recall_list_df_['click_timestamp'].apply(lambda x: 0.0 if np.isnan(x) else 1.0)\n",
    "    del recall_list_df_['click_timestamp']\n",
    "    \n",
    "    return recall_list_df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_recall_item_label_df(click_trn_hist, click_val_hist, click_tst_hist,click_trn_last, click_val_last, recall_list_df):\n",
    "    # 获取训练数据的召回列表\n",
    "    trn_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_trn_hist['user_id'].unique())]\n",
    "    # 训练数据打标签\n",
    "    trn_user_item_label_df = get_rank_label_df(trn_user_items_df, click_trn_last, is_test=False)\n",
    "    # 训练数据负采样\n",
    "    trn_user_item_label_df = neg_sample_recall_data(trn_user_item_label_df)\n",
    "    \n",
    "    if click_val is not None:\n",
    "        val_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_val_hist['user_id'].unique())]\n",
    "        val_user_item_label_df = get_rank_label_df(val_user_items_df, click_val_last, is_test=False)\n",
    "        val_user_item_label_df = neg_sample_recall_data(val_user_item_label_df)\n",
    "    else:\n",
    "        val_user_item_label_df = None\n",
    "        \n",
    "    # 测试数据不需要进行负采样，直接对所有的召回商品进行打-1标签\n",
    "    tst_user_items_df = recall_list_df[recall_list_df['user_id'].isin(click_tst_hist['user_id'].unique())]\n",
    "    tst_user_item_label_df = get_rank_label_df(tst_user_items_df, None, is_test=True)\n",
    "    \n",
    "    return trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250000/250000 [00:02<00:00, 86613.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# 读取召回列表\n",
    "recall_list_dict = get_recall_list(save_path, single_recall_model='i2i_itemcf') # 这里只选择了单路召回的结果，也可以选择多路召回结果\n",
    "# 将召回数据转换成df\n",
    "recall_list_df = recall_dict_2_df(recall_list_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_data_num: 3 neg_data_num: 1999997 pos/neg: 1.500002250003375e-06\n"
     ]
    }
   ],
   "source": [
    "# 给训练验证数据打标签，并负采样（这一部分时间比较久）\n",
    "trn_user_item_label_df, val_user_item_label_df, tst_user_item_label_df = get_user_recall_item_label_df(click_trn_hist, \n",
    "                                                                                                       click_val_hist, \n",
    "                                                                                                       click_tst_hist,\n",
    "                                                                                                       click_trn_last, \n",
    "                                                                                                       click_val_last, \n",
    "                                                                                                       recall_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1.0\n",
       "1         1.0\n",
       "2         1.0\n",
       "3         0.0\n",
       "4         0.0\n",
       "5         0.0\n",
       "6         0.0\n",
       "7         0.0\n",
       "8         0.0\n",
       "9         0.0\n",
       "10        0.0\n",
       "11        0.0\n",
       "12        0.0\n",
       "13        0.0\n",
       "14        0.0\n",
       "15        0.0\n",
       "16        0.0\n",
       "17        0.0\n",
       "18        0.0\n",
       "19        0.0\n",
       "20        0.0\n",
       "21        0.0\n",
       "22        0.0\n",
       "23        0.0\n",
       "24        0.0\n",
       "25        0.0\n",
       "26        0.0\n",
       "27        0.0\n",
       "28        0.0\n",
       "29        0.0\n",
       "         ... \n",
       "224268    0.0\n",
       "224269    0.0\n",
       "224270    0.0\n",
       "224271    0.0\n",
       "224272    0.0\n",
       "224273    0.0\n",
       "224274    0.0\n",
       "224275    0.0\n",
       "224276    0.0\n",
       "224277    0.0\n",
       "224278    0.0\n",
       "224279    0.0\n",
       "224280    0.0\n",
       "224281    0.0\n",
       "224282    0.0\n",
       "224283    0.0\n",
       "224284    0.0\n",
       "224285    0.0\n",
       "224286    0.0\n",
       "224287    0.0\n",
       "224288    0.0\n",
       "224289    0.0\n",
       "224290    0.0\n",
       "224291    0.0\n",
       "224292    0.0\n",
       "224293    0.0\n",
       "224294    0.0\n",
       "224295    0.0\n",
       "224296    0.0\n",
       "224297    0.0\n",
       "Name: label, Length: 224298, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_user_item_label_df.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将召回数据转换成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将最终的召回的df数据转换成字典的形式做排序特征\n",
    "def make_tuple_func(group_df):\n",
    "    row_data = []\n",
    "    for name, row_df in group_df.iterrows():\n",
    "        row_data.append((row_df['sim_item'], row_df['score'], row_df['label']))\n",
    "    \n",
    "    return row_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_user_item_label_tuples = trn_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "trn_user_item_label_tuples_dict = dict(zip(trn_user_item_label_tuples['user_id'], trn_user_item_label_tuples[0]))\n",
    "\n",
    "if val_user_item_label_df is not None:\n",
    "    val_user_item_label_tuples = val_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "    val_user_item_label_tuples_dict = dict(zip(val_user_item_label_tuples['user_id'], val_user_item_label_tuples[0]))\n",
    "else:\n",
    "    val_user_item_label_tuples_dict = None\n",
    "    \n",
    "tst_user_item_label_tuples = tst_user_item_label_df.groupby('user_id').apply(make_tuple_func).reset_index()\n",
    "tst_user_item_label_tuples_dict = dict(zip(tst_user_item_label_tuples['user_id'], tst_user_item_label_tuples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用户历史行为相关特征\n",
    "对于每个用户召回的每个商品， 做特征。 具体步骤如下：\n",
    "\n",
    "* 对于每个用户， 获取最后点击的N个商品的item_id，\n",
    "* 对于该用户的每个召回商品， 计算与上面最后N次点击商品的相似度的和(最大， 最小，均值)， 时间差特征，相似性特征，字数差特征，与该用户的相似性特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面基于data做历史相关的特征\n",
    "def create_feature(users_id, recall_list, click_hist_df,  articles_info, articles_emb, user_emb=None, N=1):\n",
    "    \"\"\"\n",
    "    基于用户的历史行为做相关特征\n",
    "    :param users_id: 用户id\n",
    "    :param recall_list: 对于每个用户召回的候选文章列表\n",
    "    :param click_hist_df: 用户的历史点击信息\n",
    "    :param articles_info: 文章信息\n",
    "    :param articles_emb: 文章的embedding向量, 这个可以用item_content_emb, item_w2v_emb, item_youtube_emb\n",
    "    :param user_emb: 用户的embedding向量， 这个是user_youtube_emb, 如果没有也可以不用， 但要注意如果要用的话， articles_emb就要用item_youtube_emb的形式， 这样维度才一样\n",
    "    :param N: 最近的N次点击  由于testA日志里面很多用户只存在一次历史点击， 所以为了不产生空值，默认是1\n",
    "    \"\"\"\n",
    "    \n",
    "    # 建立一个二维列表保存结果， 后面要转成DataFrame\n",
    "    all_user_feas = []\n",
    "    i = 0\n",
    "    for user_id in tqdm(users_id):\n",
    "        # 该用户的最后N次点击\n",
    "        hist_user_items = click_hist_df[click_hist_df['user_id']==user_id]['click_article_id'][-N:]\n",
    "        \n",
    "        # 遍历该用户的召回列表\n",
    "        for rank, (article_id, score, label) in enumerate(recall_list[user_id]):\n",
    "            # 该文章建立时间, 字数\n",
    "            a_create_time = articles_info[articles_info['article_id']==article_id]['created_at_ts'].values[0]\n",
    "            a_words_count = articles_info[articles_info['article_id']==article_id]['words_count'].values[0]\n",
    "            single_user_fea = [user_id, article_id]\n",
    "            # 计算与最后点击的商品的相似度的和， 最大值和最小值， 均值\n",
    "            sim_fea = []\n",
    "            time_fea = []\n",
    "            word_fea = []\n",
    "            # 遍历用户的最后N次点击文章\n",
    "            for hist_item in hist_user_items:\n",
    "                b_create_time = articles_info[articles_info['article_id']==hist_item]['created_at_ts'].values[0]\n",
    "                b_words_count = articles_info[articles_info['article_id']==hist_item]['words_count'].values[0]\n",
    "                \n",
    "                sim_fea.append(np.dot(articles_emb[hist_item], articles_emb[article_id]))\n",
    "                time_fea.append(abs(a_create_time-b_create_time))\n",
    "                word_fea.append(abs(a_words_count-b_words_count))\n",
    "                \n",
    "            single_user_fea.extend(sim_fea)      # 相似性特征\n",
    "            single_user_fea.extend(time_fea)    # 时间差特征\n",
    "            single_user_fea.extend(word_fea)    # 字数差特征\n",
    "            single_user_fea.extend([max(sim_fea), min(sim_fea), sum(sim_fea), sum(sim_fea) / len(sim_fea)])  # 相似性的统计特征\n",
    "            \n",
    "            if user_emb:  # 如果用户向量有的话， 这里计算该召回文章与用户的相似性特征 \n",
    "                single_user_fea.append(np.dot(user_emb[user_id], articles_emb[article_id]))\n",
    "                \n",
    "            single_user_fea.extend([score, rank, label])    \n",
    "            # 加入到总的表中\n",
    "            all_user_feas.append(single_user_fea)\n",
    "    \n",
    "    # 定义列名\n",
    "    id_cols = ['user_id', 'click_article_id']\n",
    "    sim_cols = ['sim' + str(i) for i in range(N)]\n",
    "    time_cols = ['time_diff' + str(i) for i in range(N)]\n",
    "    word_cols = ['word_diff' + str(i) for i in range(N)]\n",
    "    sat_cols = ['sim_max', 'sim_min', 'sim_sum', 'sim_mean']\n",
    "    user_item_sim_cols = ['user_item_sim'] if user_emb else []\n",
    "    user_score_rank_label = ['score', 'rank', 'label']\n",
    "    cols = id_cols + sim_cols + time_cols + word_cols + sat_cols + user_item_sim_cols + user_score_rank_label\n",
    "            \n",
    "    # 转成DataFrame\n",
    "    df = pd.DataFrame( all_user_feas, columns=cols)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  5.55 Mb (50.0% reduction),time spend:0.00 min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 17:22:47,415:INFO:collecting all words and their counts\n",
      "2020-12-03 17:22:47,415:INFO:PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-12-03 17:22:47,424:INFO:PROGRESS: at sentence #10000, processed 25727 words, keeping 3473 word types\n",
      "2020-12-03 17:22:47,435:INFO:PROGRESS: at sentence #20000, processed 53883 words, keeping 5811 word types\n",
      "2020-12-03 17:22:47,447:INFO:PROGRESS: at sentence #30000, processed 84881 words, keeping 7676 word types\n",
      "2020-12-03 17:22:47,461:INFO:PROGRESS: at sentence #40000, processed 118390 words, keeping 9297 word types\n",
      "2020-12-03 17:22:47,477:INFO:PROGRESS: at sentence #50000, processed 154179 words, keeping 10844 word types\n",
      "2020-12-03 17:22:47,494:INFO:PROGRESS: at sentence #60000, processed 192350 words, keeping 12357 word types\n",
      "2020-12-03 17:22:47,511:INFO:PROGRESS: at sentence #70000, processed 233685 words, keeping 13473 word types\n",
      "2020-12-03 17:22:47,529:INFO:PROGRESS: at sentence #80000, processed 281335 words, keeping 14939 word types\n",
      "2020-12-03 17:22:47,555:INFO:PROGRESS: at sentence #90000, processed 329973 words, keeping 16420 word types\n",
      "2020-12-03 17:22:47,576:INFO:PROGRESS: at sentence #100000, processed 379428 words, keeping 17904 word types\n",
      "2020-12-03 17:22:47,596:INFO:PROGRESS: at sentence #110000, processed 431464 words, keeping 18928 word types\n",
      "2020-12-03 17:22:47,616:INFO:PROGRESS: at sentence #120000, processed 489655 words, keeping 20157 word types\n",
      "2020-12-03 17:22:47,641:INFO:PROGRESS: at sentence #130000, processed 550375 words, keeping 21588 word types\n",
      "2020-12-03 17:22:47,668:INFO:PROGRESS: at sentence #140000, processed 613031 words, keeping 22923 word types\n",
      "2020-12-03 17:22:47,689:INFO:PROGRESS: at sentence #150000, processed 678645 words, keeping 24209 word types\n",
      "2020-12-03 17:22:47,714:INFO:PROGRESS: at sentence #160000, processed 749559 words, keeping 25743 word types\n",
      "2020-12-03 17:22:47,738:INFO:PROGRESS: at sentence #170000, processed 831064 words, keeping 27232 word types\n",
      "2020-12-03 17:22:47,770:INFO:PROGRESS: at sentence #180000, processed 914233 words, keeping 28612 word types\n",
      "2020-12-03 17:22:47,801:INFO:PROGRESS: at sentence #190000, processed 1004976 words, keeping 29699 word types\n",
      "2020-12-03 17:22:47,837:INFO:PROGRESS: at sentence #200000, processed 1112623 words, keeping 31116 word types\n",
      "2020-12-03 17:22:47,864:INFO:PROGRESS: at sentence #210000, processed 1200577 words, keeping 31798 word types\n",
      "2020-12-03 17:22:47,891:INFO:PROGRESS: at sentence #220000, processed 1285942 words, keeping 32381 word types\n",
      "2020-12-03 17:22:47,926:INFO:PROGRESS: at sentence #230000, processed 1380836 words, keeping 33131 word types\n",
      "2020-12-03 17:22:47,965:INFO:PROGRESS: at sentence #240000, processed 1498710 words, keeping 34213 word types\n",
      "2020-12-03 17:22:48,009:INFO:collected 35380 word types from a corpus of 1630633 raw words and 250000 sentences\n",
      "2020-12-03 17:22:48,010:INFO:Loading a fresh vocabulary\n",
      "2020-12-03 17:22:48,075:INFO:effective_min_count=1 retains 35380 unique words (100% of original 35380, drops 0)\n",
      "2020-12-03 17:22:48,076:INFO:effective_min_count=1 leaves 1630633 word corpus (100% of original 1630633, drops 0)\n",
      "2020-12-03 17:22:48,195:INFO:deleting the raw counts dictionary of 35380 items\n",
      "2020-12-03 17:22:48,197:INFO:sample=0.001 downsamples 73 most-common words\n",
      "2020-12-03 17:22:48,198:INFO:downsampling leaves estimated 1452855 word corpus (89.1% of prior 1630633)\n",
      "2020-12-03 17:22:48,276:INFO:estimated required memory for 35380 words and 16 dimensions: 22218640 bytes\n",
      "2020-12-03 17:22:48,276:INFO:resetting layer weights\n",
      "2020-12-03 17:22:48,607:INFO:training model with 24 workers on 35380 vocabulary and 16 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-12-03 17:22:49,712:INFO:EPOCH 1 - PROGRESS: at 83.21% examples, 1043641 words/s, in_qsize 46, out_qsize 0\n",
      "2020-12-03 17:22:49,868:INFO:worker thread finished; awaiting finish of 23 more threads\n",
      "2020-12-03 17:22:49,874:INFO:worker thread finished; awaiting finish of 22 more threads\n",
      "2020-12-03 17:22:49,881:INFO:worker thread finished; awaiting finish of 21 more threads\n",
      "2020-12-03 17:22:49,909:INFO:worker thread finished; awaiting finish of 20 more threads\n",
      "2020-12-03 17:22:49,923:INFO:worker thread finished; awaiting finish of 19 more threads\n",
      "2020-12-03 17:22:49,926:INFO:worker thread finished; awaiting finish of 18 more threads\n",
      "2020-12-03 17:22:49,951:INFO:worker thread finished; awaiting finish of 17 more threads\n",
      "2020-12-03 17:22:49,952:INFO:worker thread finished; awaiting finish of 16 more threads\n",
      "2020-12-03 17:22:49,953:INFO:worker thread finished; awaiting finish of 15 more threads\n",
      "2020-12-03 17:22:49,982:INFO:worker thread finished; awaiting finish of 14 more threads\n",
      "2020-12-03 17:22:49,987:INFO:worker thread finished; awaiting finish of 13 more threads\n",
      "2020-12-03 17:22:50,010:INFO:worker thread finished; awaiting finish of 12 more threads\n",
      "2020-12-03 17:22:50,011:INFO:worker thread finished; awaiting finish of 11 more threads\n",
      "2020-12-03 17:22:50,025:INFO:worker thread finished; awaiting finish of 10 more threads\n",
      "2020-12-03 17:22:50,036:INFO:worker thread finished; awaiting finish of 9 more threads\n",
      "2020-12-03 17:22:50,038:INFO:worker thread finished; awaiting finish of 8 more threads\n",
      "2020-12-03 17:22:50,048:INFO:worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-03 17:22:50,051:INFO:worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-03 17:22:50,053:INFO:worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-03 17:22:50,054:INFO:worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-03 17:22:50,057:INFO:worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-03 17:22:50,058:INFO:worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-03 17:22:50,069:INFO:worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-03 17:22:50,071:INFO:worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-03 17:22:50,072:INFO:EPOCH - 1 : training on 1630633 raw words (1452744 effective words) took 1.4s, 1058032 effective words/s\n",
      "2020-12-03 17:22:50,073:INFO:training on a 1630633 raw words (1452744 effective words) took 1.5s, 991799 effective words/s\n",
      "2020-12-03 17:22:50,073:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item_youtube_emb.pkl 文件不存在...\n",
      "user_youtube_emb.pkl 文件不存在...\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'item_youtube_emb_dict' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-4c68d7511b01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0marticle_info_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_article_info_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mall_click\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclick_trn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclick_tst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mitem_content_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_w2v_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_youtube_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_youtube_emb_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_click\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-27da4d04ba6e>\u001b[0m in \u001b[0;36mget_embedding\u001b[1;34m(save_path, all_click_df)\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_youtube_emb.pkl 文件不存在...'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mitem_content_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_w2v_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_youtube_emb_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_youtube_emb_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'item_youtube_emb_dict' referenced before assignment"
     ]
    }
   ],
   "source": [
    "article_info_df = get_article_info_df()\n",
    "all_click = click_trn.append(click_tst)\n",
    "item_content_emb_dict, item_w2v_emb_dict, item_youtube_emb_dict, user_youtube_emb_dict = get_embedding(save_path, all_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'item_content_emb_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ff753f714857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 获取训练验证及测试数据中召回列文章相关特征\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n\u001b[1;32m----> 3\u001b[1;33m                                             click_trn_hist, article_info_df, item_content_emb_dict)\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mval_user_item_label_tuples_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'item_content_emb_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# 获取训练验证及测试数据中召回列文章相关特征\n",
    "trn_user_item_feats_df = create_feature(trn_user_item_label_tuples_dict.keys(), trn_user_item_label_tuples_dict, \\\n",
    "                                            click_trn_hist, article_info_df, item_content_emb_dict)\n",
    "\n",
    "if val_user_item_label_tuples_dict is not None:\n",
    "    val_user_item_feats_df = create_feature(val_user_item_label_tuples_dict.keys(), val_user_item_label_tuples_dict, \\\n",
    "                                                click_val_hist, article_info_df, item_content_emb_dict)\n",
    "else:\n",
    "    val_user_item_feats_df = None\n",
    "    \n",
    "tst_user_item_feats_df = create_feature(tst_user_item_label_tuples_dict.keys(), tst_user_item_label_tuples_dict, \\\n",
    "                                            click_tst_hist, article_info_df, item_content_emb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trn_user_item_feats_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-f5471c573ba3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 保存一份省的每次都要重新跑，每次跑的时间都比较长\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrn_user_item_feats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'trn_user_item_feats_df.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mval_user_item_feats_df\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mval_user_item_feats_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msave_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'val_user_item_feats_df.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trn_user_item_feats_df' is not defined"
     ]
    }
   ],
   "source": [
    "# 保存一份省的每次都要重新跑，每次跑的时间都比较长\n",
    "trn_user_item_feats_df.to_csv(save_path + 'trn_user_item_feats_df.csv', index=False)\n",
    "\n",
    "if val_user_item_feats_df is not None:\n",
    "    val_user_item_feats_df.to_csv(save_path + 'val_user_item_feats_df.csv', index=False)\n",
    "\n",
    "tst_user_item_feats_df.to_csv(save_path + 'tst_user_item_feats_df.csv', index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>click_timestamp</th>\n",
       "      <th>click_environment</th>\n",
       "      <th>click_deviceGroup</th>\n",
       "      <th>click_os</th>\n",
       "      <th>click_country</th>\n",
       "      <th>click_region</th>\n",
       "      <th>click_referrer_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>249999</td>\n",
       "      <td>160974</td>\n",
       "      <td>1506959142820</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>249999</td>\n",
       "      <td>160417</td>\n",
       "      <td>1506959172820</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>249998</td>\n",
       "      <td>160974</td>\n",
       "      <td>1506959056066</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>249998</td>\n",
       "      <td>202557</td>\n",
       "      <td>1506959086066</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249997</td>\n",
       "      <td>183665</td>\n",
       "      <td>1506959088613</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  click_article_id  click_timestamp  click_environment  \\\n",
       "0   249999            160974    1506959142820                  4   \n",
       "1   249999            160417    1506959172820                  4   \n",
       "2   249998            160974    1506959056066                  4   \n",
       "3   249998            202557    1506959086066                  4   \n",
       "4   249997            183665    1506959088613                  4   \n",
       "\n",
       "   click_deviceGroup  click_os  click_country  click_region  \\\n",
       "0                  1        17              1            13   \n",
       "1                  1        17              1            13   \n",
       "2                  1        12              1            13   \n",
       "3                  1        12              1            13   \n",
       "4                  1        17              1            15   \n",
       "\n",
       "   click_referrer_type  \n",
       "0                    2  \n",
       "1                    2  \n",
       "2                    2  \n",
       "3                    2  \n",
       "4                    5  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "click_tst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Mem. usage decreased to  5.55 Mb (50.0% reduction),time spend:0.00 min\n",
      "-- Mem. usage decreased to 46.65 Mb (62.5% reduction),time spend:0.01 min\n"
     ]
    }
   ],
   "source": [
    "# 读取文章特征\n",
    "articles =  pd.read_csv(data_path+'articles.csv')\n",
    "articles = reduce_mem(articles)\n",
    "\n",
    "# 日志数据，就是前面的所有数据\n",
    "if click_val is not None:\n",
    "    all_data = click_trn.append(click_val)\n",
    "all_data = click_trn.append(click_tst)\n",
    "all_data = reduce_mem(all_data)\n",
    "\n",
    "# 拼上文章信息\n",
    "all_data = all_data.merge(articles, left_on='click_article_id', right_on='article_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1630633, 13)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def active_level(all_data, cols):\n",
    "    \"\"\"\n",
    "    制作区分用户活跃度的特征\n",
    "    :param all_data: 数据集\n",
    "    :param cols: 用到的特征列\n",
    "    \"\"\"\n",
    "    data = all_data[cols]\n",
    "    data.sort_values(['user_id', 'click_timestamp'], inplace=True)\n",
    "    user_act = pd.DataFrame(data.groupby('user_id', as_index=False)[['click_article_id', 'click_timestamp']].\\\n",
    "                            agg({'click_article_id':np.size, 'click_timestamp': {list}}).values, columns=['user_id', 'click_size', 'click_timestamp'])\n",
    "    \n",
    "    # 计算时间间隔的均值\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "        \n",
    "    user_act['time_diff_mean'] = user_act['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "    \n",
    "    # 点击次数取倒数\n",
    "    user_act['click_size'] = 1 / user_act['click_size']\n",
    "    \n",
    "    # 两者归一化\n",
    "    user_act['click_size'] = (user_act['click_size'] - user_act['click_size'].min()) / (user_act['click_size'].max() - user_act['click_size'].min())\n",
    "    user_act['time_diff_mean'] = (user_act['time_diff_mean'] - user_act['time_diff_mean'].min()) / (user_act['time_diff_mean'].max() - user_act['time_diff_mean'].min())     \n",
    "    user_act['active_level'] = user_act['click_size'] + user_act['time_diff_mean']\n",
    "    \n",
    "    user_act['user_id'] = user_act['user_id'].astype('int')\n",
    "    del user_act['click_timestamp']\n",
    "    \n",
    "    return user_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_act_fea = active_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>click_size</th>\n",
       "      <th>time_diff_mean</th>\n",
       "      <th>active_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.499515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.499515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.499515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.499515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.499466</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.499515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id click_size  time_diff_mean active_level\n",
       "0        0   0.499466        0.000048     0.499515\n",
       "1        1   0.499466        0.000048     0.499515\n",
       "2        2   0.499466        0.000048     0.499515\n",
       "3        3   0.499466        0.000048     0.499515\n",
       "4        4   0.499466        0.000048     0.499515"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_act_fea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_level(all_data, cols):\n",
    "    \"\"\"\n",
    "    制作衡量文章热度的特征\n",
    "    :param all_data: 数据集\n",
    "    :param cols: 用到的特征列\n",
    "    \"\"\"\n",
    "    data = all_data[cols]\n",
    "    data.sort_values(['click_article_id', 'click_timestamp'], inplace=True)\n",
    "    article_hot = pd.DataFrame(data.groupby('click_article_id', as_index=False)[['user_id', 'click_timestamp']].\\\n",
    "                               agg({'user_id':np.size, 'click_timestamp': {list}}).values, columns=['click_article_id', 'user_num', 'click_timestamp'])\n",
    "    \n",
    "    # 计算被点击时间间隔的均值\n",
    "    def time_diff_mean(l):\n",
    "        if len(l) == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return np.mean([j-i for i, j in list(zip(l[:-1], l[1:]))])\n",
    "        \n",
    "    article_hot['time_diff_mean'] = article_hot['click_timestamp'].apply(lambda x: time_diff_mean(x))\n",
    "    \n",
    "    # 点击次数取倒数\n",
    "    article_hot['user_num'] = 1 / article_hot['user_num']\n",
    "    \n",
    "    # 两者归一化\n",
    "    article_hot['user_num'] = (article_hot['user_num'] - article_hot['user_num'].min()) / (article_hot['user_num'].max() - article_hot['user_num'].min())\n",
    "    article_hot['time_diff_mean'] = (article_hot['time_diff_mean'] - article_hot['time_diff_mean'].min()) / (article_hot['time_diff_mean'].max() - article_hot['time_diff_mean'].min())     \n",
    "    article_hot['hot_level'] = article_hot['user_num'] + article_hot['time_diff_mean']\n",
    "    \n",
    "    article_hot['click_article_id'] = article_hot['click_article_id'].astype('int')\n",
    "    \n",
    "    del article_hot['click_timestamp']\n",
    "    \n",
    "    return article_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_hot_fea = hot_level(all_data, ['user_id', 'click_article_id', 'click_timestamp'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>click_article_id</th>\n",
       "      <th>user_num</th>\n",
       "      <th>time_diff_mean</th>\n",
       "      <th>hot_level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   click_article_id user_num  time_diff_mean hot_level\n",
       "0                 3        1             0.0         1\n",
       "1                69        1             0.0         1\n",
       "2                84        1             0.0         1\n",
       "3                94        1             0.0         1\n",
       "4               125        1             0.0         1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_hot_fea.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def device_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    制作用户的设备特征\n",
    "    :param all_data: 数据集\n",
    "    :param cols: 用到的特征列\n",
    "    \"\"\"\n",
    "    user_device_info = all_data[cols]\n",
    "    \n",
    "    # 用众数来表示每个用户的设备信息\n",
    "    user_device_info = user_device_info.groupby('user_id').agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "    \n",
    "    return user_device_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-cbe60fc75e6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 设备特征(这里时间会比较长)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdevice_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_environment'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_deviceGroup'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_os'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_country'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_region'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'click_referrer_type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0muser_device_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdevice_fea\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-74bf8a629be1>\u001b[0m in \u001b[0;36mdevice_fea\u001b[1;34m(all_data, cols)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 用众数来表示每个用户的设备信息\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0muser_device_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4654\u001b[0m         axis=''))\n\u001b[0;32m   4655\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4656\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataFrameGroupBy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4658\u001b[0m     \u001b[0magg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maggregate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, arg, *args, **kwargs)\u001b[0m\n\u001b[0;32m   4100\u001b[0m                     \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0margs\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4101\u001b[0m                     result = self._aggregate_multiple_funcs(\n\u001b[1;32m-> 4102\u001b[1;33m                         [arg], _level=_level, _axis=self.axis)\n\u001b[0m\u001b[0;32m   4103\u001b[0m                     result.columns = Index(\n\u001b[0;32m   4104\u001b[0m                         \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[1;34m(self, arg, _level, _axis)\u001b[0m\n\u001b[0;32m    595\u001b[0m                     colg = self._gotitem(col, ndim=1,\n\u001b[0;32m    596\u001b[0m                                          subset=obj.iloc[:, index])\n\u001b[1;32m--> 597\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func_or_funcs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3483\u001b[0m             \u001b[1;31m# but not the class list / tuple itself.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3484\u001b[0m             ret = self._aggregate_multiple_funcs(func_or_funcs,\n\u001b[1;32m-> 3485\u001b[1;33m                                                  (_level or 0) + 1)\n\u001b[0m\u001b[0;32m   3486\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3487\u001b[0m             \u001b[0mcyfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_cython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[1;34m(self, arg, _level)\u001b[0m\n\u001b[0;32m   3556\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3557\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_selection\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3558\u001b[1;33m             \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3560\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36maggregate\u001b[1;34m(self, func_or_funcs, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3494\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3495\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3496\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3497\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate_named\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_or_funcs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_python_agg_general\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterate_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1068\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1069\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_cast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36magg_series\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   2668\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2670\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate_series_fast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2671\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2672\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_aggregate_series_pure_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m_aggregate_series_fast\u001b[1;34m(self, obj, func)\u001b[0m\n\u001b[0;32m   2688\u001b[0m         grouper = reduction.SeriesGrouper(obj, func, group_index, ngroups,\n\u001b[0;32m   2689\u001b[0m                                           dummy)\n\u001b[1;32m-> 2690\u001b[1;33m         \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2691\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.SeriesGrouper.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\reduction.pyx\u001b[0m in \u001b[0;36mpandas._libs.reduction.SeriesGrouper.get_result\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\groupby\\groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_agg_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_builtin_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1062\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1063\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1064\u001b[0m         \u001b[1;31m# iterate through \"columns\" ex exclusions to populate output dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-39-74bf8a629be1>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# 用众数来表示每个用户的设备信息\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0muser_device_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36mvalue_counts\u001b[1;34m(self, normalize, sort, ascending, bins, dropna)\u001b[0m\n\u001b[0;32m   1036\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mvalue_counts\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1037\u001b[0m         result = value_counts(self, sort=sort, ascending=ascending,\n\u001b[1;32m-> 1038\u001b[1;33m                               normalize=normalize, bins=bins, dropna=dropna)\n\u001b[0m\u001b[0;32m   1039\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mvalue_counts\u001b[1;34m(values, sort, ascending, normalize, bins, dropna)\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msort\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\software\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36msort_values\u001b[1;34m(self, axis, ascending, inplace, kind, na_position)\u001b[0m\n\u001b[0;32m   2494\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'invalid na_position: {!r}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mna_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msortedIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msortedIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 设备特征(这里时间会比较长)\n",
    "device_cols = ['user_id', 'click_environment', 'click_deviceGroup', 'click_os', 'click_country', 'click_region', 'click_referrer_type']\n",
    "user_device_info = device_fea(all_data, device_cols)\n",
    "user_device_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_time_hob_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    制作用户的时间习惯特征\n",
    "    :param all_data: 数据集\n",
    "    :param cols: 用到的特征列\n",
    "    \"\"\"\n",
    "    user_time_hob_info = all_data[cols]\n",
    "    \n",
    "    # 先把时间戳进行归一化\n",
    "    mm = MinMaxScaler()\n",
    "    user_time_hob_info['click_timestamp'] = mm.fit_transform(user_time_hob_info[['click_timestamp']])\n",
    "    user_time_hob_info['created_at_ts'] = mm.fit_transform(user_time_hob_info[['created_at_ts']])\n",
    "\n",
    "    user_time_hob_info = user_time_hob_info.groupby('user_id').agg('mean').reset_index()\n",
    "    \n",
    "    user_time_hob_info.rename(columns={'click_timestamp': 'user_time_hob1', 'created_at_ts': 'user_time_hob2'}, inplace=True)\n",
    "    return user_time_hob_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_time_hob_cols = ['user_id', 'click_timestamp', 'created_at_ts']\n",
    "user_time_hob_info = user_time_hob_fea(all_data, user_time_hob_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_cat_hob_fea(all_data, cols):\n",
    "    \"\"\"\n",
    "    用户的主题爱好\n",
    "    :param all_data: 数据集\n",
    "    :param cols: 用到的特征列\n",
    "    \"\"\"\n",
    "    user_category_hob_info = all_data[cols]\n",
    "    user_category_hob_info = user_category_hob_info.groupby('user_id').agg({list}).reset_index()\n",
    "    \n",
    "    user_cat_hob_info = pd.DataFrame()\n",
    "    user_cat_hob_info['user_id'] = user_category_hob_info['user_id']\n",
    "    user_cat_hob_info['cate_list'] = user_category_hob_info['category_id']\n",
    "    \n",
    "    return user_cat_hob_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_category_hob_cols = ['user_id', 'category_id']\n",
    "user_cat_hob_info = user_cat_hob_fea(all_data, user_category_hob_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_wcou_info = all_data.groupby('user_id')['words_count'].agg('mean').reset_index()\n",
    "user_wcou_info.rename(columns={'words_count': 'words_hbo'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'user_device_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-336bc82c39b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 所有表进行合并\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0muser_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_act_fea\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muser_device_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0muser_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_time_hob_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0muser_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_cat_hob_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0muser_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0muser_info\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_wcou_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'user_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'user_device_info' is not defined"
     ]
    }
   ],
   "source": [
    "# 所有表进行合并\n",
    "user_info = pd.merge(user_act_fea, user_device_info, on='user_id')\n",
    "user_info = user_info.merge(user_time_hob_info, on='user_id')\n",
    "user_info = user_info.merge(user_cat_hob_info, on='user_id')\n",
    "user_info = user_info.merge(user_wcou_info, on='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
